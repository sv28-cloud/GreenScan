{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b42a257-d07f-4049-b80b-2a4341f789f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Imports & Dataset Loading\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7133ee0-29cc-40fb-87d2-4af735a934ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c294ba-0dab-40d7-ac0f-e9966519fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset root folder (each subfolder = class)\n",
    "dataset_path = r\"garbage-dataset\"  # <-- change this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeef432-90cb-4b09-99d0-86bdc1dc3ce9",
   "metadata": {},
   "source": [
    "Downloads/GreenScan/AiModel/garbage-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47cea9-0655-455a-9a0c-e29eac372dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed54ed-3dcb-4a4b-9bf7-019150948e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check folders\n",
    "# Build a dictionary of DataFrames (one per category)\n",
    "categories = [f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))]\n",
    "print(f\"Found {len(categories)} categories:\", categories)\n",
    "\n",
    "category_dfs = {}\n",
    "\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(dataset_path, category)\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    data = []\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            data.append({\"path\": img_path, \"image\": img})\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading {img_file}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    category_dfs[category] = df\n",
    "    print(f\"âœ… Loaded {len(df)} images for class '{category}'\")\n",
    "\n",
    "# Show one random image per category\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, category in zip(axes, categories):\n",
    "    img = random.choice(category_dfs[category][\"image\"])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(category)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5813882-84c4-473a-8c94-dcfb4bc92bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "base_dir = \"./garbage-dataset\"\n",
    "\n",
    "categories = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "print(f\"Found {len(categories)} categories: {categories}\")\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for category in categories:\n",
    "    folder = os.path.join(base_dir, category)\n",
    "    image_paths = [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    df = pd.DataFrame({'image_path': image_paths})\n",
    "    dataframes[category] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aca71ae-27ec-4b80-8239-336ac90cd968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ðŸ§© Step 2: Image Preprocessing + Embedding Extraction\n",
    "# ========================================\n",
    "\n",
    "# --- Define Preprocessing Transform ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # standard input size for ResNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# --- Load Pretrained Model (ResNet50) ---\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # remove final classification layer\n",
    "resnet.eval()\n",
    "\n",
    "# --- Move model to GPU if available ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# --- Helper Function to Extract Embedding ---\n",
    "def get_embedding(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = resnet(image).squeeze().cpu().numpy()\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return np.zeros(2048)  # ResNet50 outputs 2048-dim vector\n",
    "\n",
    "# --- Generate Embeddings for Each Category ---\n",
    "embedding_tables = {}\n",
    "\n",
    "for category, df in dataframes.items():\n",
    "    print(f\"ðŸ” Extracting embeddings for category: {category} ({len(df)} images)\")\n",
    "    df['embedding'] = df['image_path'].apply(get_embedding)\n",
    "    embedding_tables[category] = df\n",
    "\n",
    "print(\"âœ… Embeddings extracted successfully!\")\n",
    "\n",
    "# --- Optional: Save intermediate embedding data ---\n",
    "for cat, df in embedding_tables.items():\n",
    "    df.to_pickle(f'embeddings_{cat}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11ce13-a46c-4c09-bb6c-8941e4288bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label coloum to embeddings\n",
    "import pandas as pd, glob, os\n",
    "import numpy as np\n",
    "\n",
    "# --- Load all embedding tables ---\n",
    "pkl_files = glob.glob(\"embeddings/*.pkl\")\n",
    "all_embeddings = []\n",
    "\n",
    "for file in pkl_files:\n",
    "    category = os.path.basename(file).replace(\"embeddings_\", \"\").replace(\".pkl\", \"\")\n",
    "    df = pd.read_pickle(file)\n",
    "    df['label'] = category  # Assign category label\n",
    "    all_embeddings.append(df)\n",
    "\n",
    "# Combine all embeddings into one DataFrame\n",
    "all_embeddings_df = pd.concat(all_embeddings, ignore_index=True)\n",
    "\n",
    "# --- Convert embeddings to matrix ---\n",
    "embedding_matrix = np.vstack(all_embeddings_df['embedding'].values)\n",
    "embedding_df = pd.DataFrame(embedding_matrix, columns=[f'f_{i}' for i in range(embedding_matrix.shape[1])])\n",
    "\n",
    "# Add labels\n",
    "embedding_df['label'] = all_embeddings_df['label']\n",
    "\n",
    "# --- Save to CSV ---\n",
    "embedding_df.to_csv(\"embeddings.csv\", index=False)\n",
    "print(\"âœ… Saved embeddings.csv with shape:\", embedding_df.shape)\n",
    "print(\"Categories:\", embedding_df['label'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a222b4a-077b-4953-a696-b4b2defbb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, glob\n",
    "\n",
    "pkl_files = glob.glob(\"embeddings/*.pkl\")\n",
    "print(\"Found:\", len(pkl_files), \"files\")\n",
    "\n",
    "# Look at the first one\n",
    "df = pd.read_pickle(pkl_files[0])\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e291ea-e469-411d-93da-6dc3b1b1308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ðŸ§© Step 3: Combine All Embeddings into One Table\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- Load all embedding pickle files ---\n",
    "pkl_files = glob.glob(\"embeddings/*.pkl\")\n",
    "all_embeddings = []\n",
    "\n",
    "for file in pkl_files:\n",
    "    # Extract the category name from filename, e.g. \"embeddings_battery.pkl\" -> \"battery\"\n",
    "    category = os.path.basename(file).replace(\"embeddings_\", \"\").replace(\".pkl\", \"\")\n",
    "    \n",
    "    # Load the DataFrame\n",
    "    df = pd.read_pickle(file)\n",
    "    \n",
    "    # Add label column\n",
    "    df['label'] = category\n",
    "    \n",
    "    # Store it\n",
    "    all_embeddings.append(df)\n",
    "\n",
    "# --- Combine all categories into one DataFrame ---\n",
    "all_embeddings_df = pd.concat(all_embeddings, ignore_index=True)\n",
    "\n",
    "# --- Build numeric embedding matrix ---\n",
    "embedding_matrix = np.vstack(all_embeddings_df['embedding'].values)\n",
    "embedding_df = pd.DataFrame(embedding_matrix, columns=[f'f_{i}' for i in range(embedding_matrix.shape[1])])\n",
    "\n",
    "# --- Attach labels ---\n",
    "embedding_df['label'] = all_embeddings_df['label']\n",
    "\n",
    "# --- Save final CSV ---\n",
    "embedding_df.to_csv(\"embeddings.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Saved embeddings.csv with shape:\", embedding_df.shape)\n",
    "print(\"Categories found:\", embedding_df['label'].unique())\n",
    "print(embedding_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9b8d9-1415-4b41-928d-e55cc0e30a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: K-Means Clustering & Validation ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    confusion_matrix,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Load embeddings ---\n",
    "df = pd.read_csv(\"embeddings.csv\")\n",
    "\n",
    "# --- Prepare data and labels ---\n",
    "X = df.drop(columns=[\"label\"]).values\n",
    "y_true = LabelEncoder().fit_transform(df[\"label\"])\n",
    "\n",
    "# --- KMeans clustering ---\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=20)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# --- Metrics ---\n",
    "ari = adjusted_rand_score(y_true, y_pred)\n",
    "nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "h, c, v = homogeneity_completeness_v_measure(y_true, y_pred)\n",
    "\n",
    "# --- Align cluster labels to true labels for crude accuracy ---\n",
    "# (optional heuristic: find best label for each cluster)\n",
    "from scipy.stats import mode\n",
    "labels_map = {}\n",
    "for i in range(10):\n",
    "    mask = (y_pred == i)\n",
    "    if np.any(mask):\n",
    "        labels_map[i] = mode(y_true[mask], keepdims=True)[0][0]\n",
    "    else:\n",
    "        labels_map[i] = -1\n",
    "\n",
    "mapped_preds = np.array([labels_map[c] for c in y_pred])\n",
    "acc = accuracy_score(y_true, mapped_preds)\n",
    "\n",
    "print(f\"âœ… Clustering Metrics:\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Info (NMI): {nmi:.4f}\")\n",
    "print(f\"Homogeneity: {h:.4f}, Completeness: {c:.4f}, V-measure: {v:.4f}\")\n",
    "print(f\"Approximate Accuracy: {acc:.4f}\")\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_true, mapped_preds)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix (True vs KMeans Mapped)\")\n",
    "plt.xlabel(\"Predicted Cluster\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# --- t-SNE Visualization ---\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X[:1000])  # limit to avoid slowdown\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y_true[:1000], cmap=\"tab10\", s=10)\n",
    "plt.title(\"t-SNE: True Labels\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y_pred[:1000], cmap=\"tab10\", s=10)\n",
    "plt.title(\"t-SNE: KMeans Clusters\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Feature Correlation Heatmap (optional) ---\n",
    "corr = np.corrcoef(X[:200, :50].T)  # subset for speed\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Feature Correlation Heatmap (sampled)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053f2f6-3e69-4c81-969a-3f310375049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Validation & Insights\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import os\n",
    "\n",
    "# --- Compute per-class precision, recall, F1 ---\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, mapped_preds, labels=np.unique(y_true))\n",
    "\n",
    "# Create DataFrame for easy saving & plotting\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"class_id\": np.unique(y_true),\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1_score\": f1,\n",
    "    \"support\": support\n",
    "})\n",
    "\n",
    "# Optional: map class ids back to original labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df[\"label\"])\n",
    "metrics_df[\"class_name\"] = label_encoder.inverse_transform(metrics_df[\"class_id\"])\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_df.to_csv(\"kmeans_per_class_metrics.csv\", index=False)\n",
    "print(\"âœ… Per-class metrics saved to 'kmeans_per_class_metrics.csv'\")\n",
    "metrics_df\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "x = np.arange(len(metrics_df))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, metrics_df[\"precision\"], width, label=\"Precision\")\n",
    "plt.bar(x, metrics_df[\"recall\"], width, label=\"Recall\")\n",
    "plt.bar(x + width, metrics_df[\"f1_score\"], width, label=\"F1 Score\")\n",
    "\n",
    "plt.xticks(x, metrics_df[\"class_name\"], rotation=45)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Per-Class Precision, Recall, F1\")\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Thresholds for overlap (adjust as needed)\n",
    "low_precision_classes = metrics_df[metrics_df[\"precision\"] < 0.7]\n",
    "low_recall_classes = metrics_df[metrics_df[\"recall\"] < 0.7]\n",
    "\n",
    "for i, p in enumerate(precision):\n",
    "    if p == 0:\n",
    "        print(f\"âš ï¸ Class {label_encoder.inverse_transform([i])[0]} had no predicted samples\")\n",
    "\n",
    "print(\"âš ï¸ Classes with low precision (likely overlaps in clusters):\")\n",
    "print(low_precision_classes[[\"class_name\",\"precision\",\"recall\",\"f1_score\"]])\n",
    "\n",
    "print(\"\\nâš ï¸ Classes with low recall (clusters capturing multiple classes):\")\n",
    "print(low_recall_classes[[\"class_name\",\"precision\",\"recall\",\"f1_score\"]])\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.makedirs(\"plots\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(x - width, metrics_df[\"precision\"], width, label=\"Precision\")\n",
    "plt.bar(x, metrics_df[\"recall\"], width, label=\"Recall\")\n",
    "plt.bar(x + width, metrics_df[\"f1_score\"], width, label=\"F1 Score\")\n",
    "plt.xticks(x, metrics_df[\"class_name\"], rotation=45)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Per-Class Precision, Recall, F1\")\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/per_class_metrics.png\")\n",
    "plt.close()\n",
    "print(\"âœ… Metrics plot saved to 'plots/per_class_metrics.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d21b26-4218-4d0f-a7e2-f3e4be579db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, map cluster IDs to original class names using your labels_map\n",
    "cluster_to_class = {i: label_encoder.inverse_transform([labels_map[i]])[0] if labels_map[i] != -1 else \"None\"\n",
    "                    for i in range(10)}\n",
    "\n",
    "# Add cluster predictions and mapped class names to the DataFrame\n",
    "df['pred_cluster'] = y_pred\n",
    "df['pred_class'] = df['pred_cluster'].map(cluster_to_class)\n",
    "\n",
    "# Filter only Plastic images\n",
    "plastic_df = df[df['label'] == 'plastic']\n",
    "\n",
    "# Count which predicted classes Plastic images ended up in\n",
    "plastic_overlap = plastic_df['pred_class'].value_counts()\n",
    "\n",
    "print(\"ðŸ“Š Plastic images distribution across predicted clusters/classes:\")\n",
    "print(plastic_overlap)\n",
    "plt.figure(figsize=(6,4))\n",
    "plastic_overlap.plot(kind='bar', color='orange')\n",
    "plt.title(\"Plastic Images Distribution Across Predicted Classes\")\n",
    "plt.ylabel(\"Number of Plastic Images\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a586287-4a21-4a2a-88a0-307b90acbd27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
